import os
import cv2
import shutil
import numpy as np
from pathlib import Path
from typing import List, Dict, Set, Tuple, Optional
from sklearn.metrics.pairwise import cosine_distances
from insightface.app import FaceAnalysis
import hdbscan
from collections import defaultdict

# =============================================================================
# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –ø–æ—Ä–æ–≥–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
# =============================================================================
DEFAULT_THRESHOLD = 0.27           # [1] –û—Å–Ω–æ–≤–Ω–æ–π –ø–æ—Ä–æ–≥ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ ‚Äî –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–π
FINAL_MERGE_THRESHOLD = 0.25      # [1] –ü–æ—Ä–æ–≥ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —á–µ—Ä–µ–∑ merge_clusters_by_centroid
POSTPROCESS_THRESHOLD = 0.23      # [1] –ü–æ—Ä–æ–≥ –≤ post_process_clusters
SMART_LARGE_THRESHOLD = 0.28      # [1] –ü–æ—Ä–æ–≥ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –º–∞–ª–µ–Ω—å–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å –±–æ–ª—å—à–∏–º–∏
SMART_SMALL_THRESHOLD = 0.25      # [1] –ü–æ—Ä–æ–≥ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –º–µ–∂–¥—É –º–∞–ª–µ–Ω—å–∫–∏–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏
SUPER_AGGRESSIVE_THRESHOLD = 0.26 # [1] –ü–æ—Ä–æ–≥ –¥–ª—è —Å—É–ø–µ—Ä-–∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
# =============================================================================

IMG_EXTS = {'.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff', '.webp'}

def is_image(p: Path) -> bool:
    return p.suffix.lower() in IMG_EXTS

def _win_long(path: Path) -> str:
    p = str(path.resolve())
    if os.name == "nt":
        return "\\\\?\\" + p if not p.startswith("\\\\?\\") else p
    return p

def imread_safe(path: Path):
    try:
        data = np.fromfile(_win_long(path), dtype=np.uint8)
        if data.size == 0:
            return None
        return cv2.imdecode(data, cv2.IMREAD_COLOR)
    except Exception:
        return None

def merge_clusters_by_centroid(
    embeddings: List[np.ndarray],
    owners: List[Path],
    raw_labels: np.ndarray,
    threshold: Optional[float] = DEFAULT_THRESHOLD,
    auto_threshold: bool = False,
    margin: float = 0.10,  # –ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
    min_threshold: float = 0.18,  # –ë–æ–ª–µ–µ –º—è–≥–∫–∏–π –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
    max_threshold: float = 0.45,  # –ë–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–π –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
    progress_callback=None
) -> Tuple[Dict[int, Set[Path]], Dict[Path, Set[int]]]:

    if progress_callback:
        progress_callback("üîÑ –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –±–ª–∏–∑–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...", 92)

    cluster_embeddings: Dict[int, List[np.ndarray]] = defaultdict(list)
    cluster_paths: Dict[int, List[Path]] = defaultdict(list)

    for label, emb, path in zip(raw_labels, embeddings, owners):
        if label == -1:
            continue
        cluster_embeddings[label].append(emb)
        cluster_paths[label].append(path)

    centroids = {label: np.mean(embs, axis=0) for label, embs in cluster_embeddings.items()}
    labels = list(centroids.keys())

    if auto_threshold and threshold is None:
        pairwise = [cosine_distances([centroids[a]], [centroids[b]])[0][0]
                    for i, a in enumerate(labels) for b in labels[i+1:]]
        if pairwise:
            mean_dist = np.mean(pairwise)
            # –ë–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
            threshold = max(min_threshold, min(mean_dist - margin, max_threshold))
        else:
            threshold = min_threshold

        if progress_callback:
            progress_callback(f"üìè –ê–≤—Ç–æ-–ø–æ—Ä–æ–≥ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è: {threshold:.3f}", 93)
    elif threshold is None:
        # –ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
        threshold = 0.35

    next_cluster_id = 0
    label_to_group = {}
    total = len(labels)

    for i, label_i in enumerate(labels):
        if progress_callback:
            percent = 93 + int((i + 1) / max(total, 1) * 2)
            progress_callback(f"üîÅ –°–ª–∏—è–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {percent}% ({i+1}/{total})", percent)

        if label_i in label_to_group:
            continue
        group = [label_i]
        for j in range(i + 1, len(labels)):
            label_j = labels[j]
            if label_j in label_to_group:
                continue
            dist = cosine_distances([centroids[label_i]], [centroids[label_j]])[0][0]
            if dist < threshold:
                group.append(label_j)

        for l in group:
            label_to_group[l] = next_cluster_id
        next_cluster_id += 1

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –≤–Ω—É—Ç—Ä–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    if progress_callback:
        progress_callback("üîó –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ—Ö–æ–∂–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...", 94)
    
    # –í—ã—á–∏—Å–ª—è–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
    cluster_max_distances = {}
    for label, embs in cluster_embeddings.items():
        if len(embs) > 1:
            distances = []
            for i in range(len(embs)):
                for j in range(i + 1, len(embs)):
                    dist = cosine_distances([embs[i]], [embs[j]])[0][0]
                    distances.append(dist)
            cluster_max_distances[label] = max(distances) if distances else 0
        else:
            cluster_max_distances[label] = 0
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã, –µ—Å–ª–∏ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –∏—Ö —Ü–µ–Ω—Ç—Ä–∞–º–∏ –º–µ–Ω—å—à–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –≤–Ω—É—Ç—Ä–∏ –ª—é–±–æ–≥–æ –∏–∑ –Ω–∏—Ö
    additional_merges = {}
    for i, label_i in enumerate(labels):
        if label_i in additional_merges:
            continue
        for j, label_j in enumerate(labels[i+1:], i+1):
            if label_j in additional_merges:
                continue
            dist = cosine_distances([centroids[label_i]], [centroids[label_j]])[0][0]
            max_internal_dist = max(cluster_max_distances[label_i], cluster_max_distances[label_j])
            
            # –ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
            if dist < max_internal_dist * 1.5:  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –±—É—Ñ–µ—Ä –¥–ª—è –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
                additional_merges[label_j] = label_i
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –¥–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –µ—â–µ –±–æ–ª–µ–µ –º—è–≥–∫–∏–µ —É—Å–ª–æ–≤–∏—è
            elif (len(cluster_embeddings[label_i]) <= 4 and len(cluster_embeddings[label_j]) <= 4 and 
                  dist < 0.38):  # –ë–æ–ª–µ–µ –º—è–≥–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
                additional_merges[label_j] = label_i
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
    for label_j, label_i in additional_merges.items():
        if label_i in label_to_group:
            label_to_group[label_j] = label_to_group[label_i]
        else:
            label_to_group[label_j] = label_to_group.get(label_i, next_cluster_id)
            if label_i not in label_to_group:
                label_to_group[label_i] = next_cluster_id
                next_cluster_id += 1
    
    # –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ: –ø–æ–≤—Ç–æ—Ä—è–µ–º –ø—Ä–æ—Ü–µ—Å—Å –¥–ª—è –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    if progress_callback:
        progress_callback("üîÑ –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...", 95)
    
    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ —Ü–µ–Ω—Ç—Ä–æ–∏–¥—ã –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
    merged_centroids = {}
    for label, group_id in label_to_group.items():
        if group_id not in merged_centroids:
            merged_centroids[group_id] = centroids[label]
        else:
            # –£—Å—Ä–µ–¥–Ω—è–µ–º —Ü–µ–Ω—Ç—Ä–æ–∏–¥—ã –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
            merged_centroids[group_id] = (merged_centroids[group_id] + centroids[label]) / 2
    
    # –ü–æ–≤—Ç–æ—Ä–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –Ω–æ–≤—ã–º–∏ —Ü–µ–Ω—Ç—Ä–æ–∏–¥–∞–º–∏
    final_merges = {}
    merged_labels = list(merged_centroids.keys())
    for i, label_i in enumerate(merged_labels):
        if label_i in final_merges:
            continue
        for j, label_j in enumerate(merged_labels[i+1:], i+1):
            if label_j in final_merges:
                continue
            dist = cosine_distances([merged_centroids[label_i]], [merged_centroids[label_j]])[0][0]
            # –ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
            if dist < FINAL_MERGE_THRESHOLD:  # –ë–æ–ª–µ–µ –º—è–≥–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
                final_merges[label_j] = label_i
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
    for label_j, label_i in final_merges.items():
        # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Å–µ –∫–ª–∞—Å—Ç–µ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ —Å–≤—è–∑–∞–Ω—ã —Å label_j
        for original_label, group_id in label_to_group.items():
            if group_id == label_j:
                label_to_group[original_label] = label_to_group.get(label_i, label_i)

    merged_clusters: Dict[int, Set[Path]] = defaultdict(set)
    cluster_by_img: Dict[Path, Set[int]] = defaultdict(set)

    for label, path in zip(raw_labels, owners):
        if label == -1:
            continue
        new_label = label_to_group[label]
        merged_clusters[new_label].add(path)
        cluster_by_img[path].add(new_label)

    return merged_clusters, cluster_by_img

def validate_cluster_quality(embeddings_list: List[np.ndarray], threshold: float = 0.4) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–∞ - –≤—Å–µ –ª–∏ –ª–∏—Ü–∞ –≤ –Ω–µ–º –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–æ—Ö–æ–∂–∏
    """
    if len(embeddings_list) <= 1:
        return True
    
    # –í—ã—á–∏—Å–ª—è–µ–º –≤—Å–µ –ø–æ–ø–∞—Ä–Ω—ã–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –≤–Ω—É—Ç—Ä–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞
    distances = []
    for i in range(len(embeddings_list)):
        for j in range(i + 1, len(embeddings_list)):
            dist = cosine_distances([embeddings_list[i]], [embeddings_list[j]])[0][0]
            distances.append(dist)
    
    # –ï—Å–ª–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –±–æ–ª—å—à–µ –ø–æ—Ä–æ–≥–∞, –∫–ª–∞—Å—Ç–µ—Ä –ø–ª–æ—Ö–æ–π
    max_distance = max(distances) if distances else 0
    return max_distance < threshold

def post_process_clusters(
    cluster_map: Dict[int, Set[Path]], 
    embeddings: List[np.ndarray], 
    owners: List[Path],
    progress_callback=None
) -> Dict[int, Set[Path]]:
    """
    –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏—Ö –ª–∏—Ü
    """
    if progress_callback:
        progress_callback("üîç –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...", 96)
    
    # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ –ø—É—Ç—å -> —ç–º–±–µ–¥–¥–∏–Ω–≥
    path_to_embedding = {}
    for emb, path in zip(embeddings, owners):
        path_to_embedding[path] = emb
    
    # –ù–∞—Ö–æ–¥–∏–º –∫–ª–∞—Å—Ç–µ—Ä—ã –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
    clusters_to_merge = []
    cluster_ids = list(cluster_map.keys())
    
    for i, cluster_id_i in enumerate(cluster_ids):
        if cluster_id_i in clusters_to_merge:
            continue
            
        paths_i = cluster_map[cluster_id_i]
        if len(paths_i) == 0:
            continue
            
        # –í—ã—á–∏—Å–ª—è–µ–º —Ü–µ–Ω—Ç—Ä–æ–∏–¥ –ø–µ—Ä–≤–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
        embeddings_i = [path_to_embedding[p] for p in paths_i if p in path_to_embedding]
        if not embeddings_i:
            continue
        centroid_i = np.mean(embeddings_i, axis=0)
        
        for j, cluster_id_j in enumerate(cluster_ids[i+1:], i+1):
            if cluster_id_j in clusters_to_merge:
                continue
                
            paths_j = cluster_map[cluster_id_j]
            if len(paths_j) == 0:
                continue
                
            # –í—ã—á–∏—Å–ª—è–µ–º —Ü–µ–Ω—Ç—Ä–æ–∏–¥ –≤—Ç–æ—Ä–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
            embeddings_j = [path_to_embedding[p] for p in paths_j if p in path_to_embedding]
            if not embeddings_j:
                continue
            centroid_j = np.mean(embeddings_j, axis=0)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É —Ü–µ–Ω—Ç—Ä–æ–∏–¥–∞–º–∏
            dist = cosine_distances([centroid_i], [centroid_j])[0][0]
            
            # –ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏
            if dist < POSTPROCESS_THRESHOLD:  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø–æ—Ä–æ–≥ –¥–ª—è –Ω–∞—á–∞–ª—å–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –≤–∞–ª–∏–¥–∏—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
                combined_embeddings = embeddings_i + embeddings_j
                
                # –ë–æ–ª–µ–µ –º—è–≥–∫–∏–µ –ø–æ—Ä–æ–≥–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
                validation_threshold = 0.45 if (len(embeddings_i) <= 3 or len(embeddings_j) <= 3) else 0.40
                
                if validate_cluster_quality(combined_embeddings, threshold=validation_threshold):
                    clusters_to_merge.append((cluster_id_i, cluster_id_j))
                    print(f"‚úÖ –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã {cluster_id_i} –∏ {cluster_id_j} (—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {dist:.3f})")
                else:
                    print(f"‚ö†Ô∏è –û—Ç–∫–ª–æ–Ω–µ–Ω–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ {cluster_id_i} –∏ {cluster_id_j} - –Ω–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ (—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {dist:.3f})")
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
    if clusters_to_merge:
        if progress_callback:
            progress_callback(f"üîó –û–±—ä–µ–¥–∏–Ω—è–µ–º {len(clusters_to_merge)} –ø–∞—Ä –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...", 97)
        
        # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–π
        merge_groups = {}
        next_group_id = 0
        
        for cluster_a, cluster_b in clusters_to_merge:
            group_a = None
            group_b = None
            
            # –ò—â–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –≥—Ä—É–ø–ø—ã
            for group_id, clusters in merge_groups.items():
                if cluster_a in clusters:
                    group_a = group_id
                if cluster_b in clusters:
                    group_b = group_id
            
            if group_a is not None and group_b is not None:
                # –û–±–µ –≥—Ä—É–ø–ø—ã —Å—É—â–µ—Å—Ç–≤—É—é—Ç - –æ–±—ä–µ–¥–∏–Ω—è–µ–º –∏—Ö
                if group_a != group_b:
                    merge_groups[group_a].extend(merge_groups[group_b])
                    del merge_groups[group_b]
            elif group_a is not None:
                # –î–æ–±–∞–≤–ª—è–µ–º cluster_b –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –≥—Ä—É–ø–ø–µ
                merge_groups[group_a].append(cluster_b)
            elif group_b is not None:
                # –î–æ–±–∞–≤–ª—è–µ–º cluster_a –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –≥—Ä—É–ø–ø–µ
                merge_groups[group_b].append(cluster_a)
            else:
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –≥—Ä—É–ø–ø—É
                merge_groups[next_group_id] = [cluster_a, cluster_b]
                next_group_id += 1
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
        final_cluster_map = {}
        used_clusters = set()
        
        # –°–Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –≥—Ä—É–ø–ø—ã
        for group_clusters in merge_groups.values():
            if not group_clusters:
                continue
                
            # –í—ã–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤—ã–π –∫–ª–∞—Å—Ç–µ—Ä –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π
            main_cluster = group_clusters[0]
            final_cluster_map[main_cluster] = set()
            
            for cluster_id in group_clusters:
                if cluster_id in cluster_map:
                    final_cluster_map[main_cluster].update(cluster_map[cluster_id])
                    used_clusters.add(cluster_id)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
        for cluster_id, paths in cluster_map.items():
            if cluster_id not in used_clusters:
                final_cluster_map[cluster_id] = paths
        
        return final_cluster_map
    
    return cluster_map

def smart_final_merge(
    cluster_map: Dict[int, Set[Path]], 
    embeddings: List[np.ndarray], 
    owners: List[Path],
    progress_callback=None
) -> Dict[int, Set[Path]]:
    """
    –§–∏–Ω–∞–ª—å–Ω–æ–µ —É–º–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–ª—è —Å–ª—É—á–∞–µ–≤, –∫–æ–≥–¥–∞ –æ–¥–∏–Ω —á–µ–ª–æ–≤–µ–∫ –ø–æ–ø–∞–ª –≤ —Ä–∞–∑–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
    """
    if progress_callback:
        progress_callback("üß† –£–º–Ω–æ–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ...", 98)
    
    # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ –ø—É—Ç—å -> —ç–º–±–µ–¥–¥–∏–Ω–≥
    path_to_embedding = {}
    for emb, path in zip(embeddings, owners):
        path_to_embedding[path] = emb
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã –ø–æ —Ä–∞–∑–º–µ—Ä—É - –º–∞–ª–µ–Ω—å–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã —á–∞—â–µ –≤—Å–µ–≥–æ –Ω—É–∂–Ω–æ –æ–±—ä–µ–¥–∏–Ω—è—Ç—å
    small_clusters = []
    large_clusters = []
    
    for cluster_id, paths in cluster_map.items():
        if len(paths) <= 3:  # –ú–∞–ª–µ–Ω—å–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
            small_clusters.append(cluster_id)
        else:
            large_clusters.append(cluster_id)
    
    print(f"üîç –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: –º–∞–ª–µ–Ω—å–∫–∏—Ö={len(small_clusters)}, –±–æ–ª—å—à–∏—Ö={len(large_clusters)}")
    
    # –ü—ã—Ç–∞–µ–º—Å—è –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –º–∞–ª–µ–Ω—å–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã —Å –±–æ–ª—å—à–∏–º–∏ –∏–ª–∏ –º–µ–∂–¥—É —Å–æ–±–æ–π
    merges_to_apply = []
    
    for small_id in small_clusters:
        small_paths = cluster_map[small_id]
        small_embeddings = [path_to_embedding[p] for p in small_paths if p in path_to_embedding]
        if not small_embeddings:
            continue
        small_centroid = np.mean(small_embeddings, axis=0)
        
        best_match = None
        best_distance = float('inf')
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –±–æ–ª—å—à–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
        for large_id in large_clusters:
            if large_id == small_id:
                continue
            large_paths = cluster_map[large_id]
            large_embeddings = [path_to_embedding[p] for p in large_paths if p in path_to_embedding]
            if not large_embeddings:
                continue
            large_centroid = np.mean(large_embeddings, axis=0)
            
            dist = cosine_distances([small_centroid], [large_centroid])[0][0]
            if dist < SMART_LARGE_THRESHOLD and dist < best_distance:  # –ï—â–µ –±–æ–ª–µ–µ –º—è–≥–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Å –±–æ–ª—å—à–∏–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏
                best_distance = dist
                best_match = large_id
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–π –±–æ–ª—å—à–æ–π –∫–ª–∞—Å—Ç–µ—Ä, –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥—Ä—É–≥–∏–µ –º–∞–ª–µ–Ω—å–∫–∏–µ
        if best_match is None:
            for other_small_id in small_clusters:
                if other_small_id == small_id or other_small_id in [m[1] for m in merges_to_apply]:
                    continue
                other_paths = cluster_map[other_small_id]
                other_embeddings = [path_to_embedding[p] for p in other_paths if p in path_to_embedding]
                if not other_embeddings:
                    continue
                other_centroid = np.mean(other_embeddings, axis=0)
                
                dist = cosine_distances([small_centroid], [other_centroid])[0][0]
                if dist < SMART_SMALL_THRESHOLD and dist < best_distance:  # –ë–æ–ª–µ–µ –º—è–≥–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –º–∞–ª–µ–Ω—å–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
                    best_distance = dist
                    best_match = other_small_id
        
        if best_match is not None:
            merges_to_apply.append((small_id, best_match))
            print(f"üîó –ü–ª–∞–Ω–∏—Ä—É–µ–º –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –∫–ª–∞—Å—Ç–µ—Ä {small_id} —Å {best_match} (—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {best_distance:.3f})")
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
    if merges_to_apply:
        print(f"üîÑ –ü—Ä–∏–º–µ–Ω—è–µ–º {len(merges_to_apply)} —É–º–Ω—ã—Ö –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–π...")
        final_cluster_map = cluster_map.copy()
        
        for source_id, target_id in merges_to_apply:
            if source_id in final_cluster_map and target_id in final_cluster_map:
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã
                final_cluster_map[target_id].update(final_cluster_map[source_id])
                del final_cluster_map[source_id]
                print(f"‚úÖ –û–±—ä–µ–¥–∏–Ω–∏–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä {source_id} —Å {target_id}")
        
        return final_cluster_map
    
    return cluster_map

def super_aggressive_merge(
    cluster_map: Dict[int, Set[Path]], 
    embeddings: List[np.ndarray], 
    owners: List[Path],
    progress_callback=None
) -> Dict[int, Set[Path]]:
    """
    –°—É–ø–µ—Ä-–∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —Å–ª–∏—è–Ω–∏—è –ø–æ—Ö–æ–∂–∏—Ö –ª–∏—Ü
    """
    if progress_callback:
        progress_callback("üî• –°—É–ø–µ—Ä-–∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ...", 99)
    
    # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ –ø—É—Ç—å -> —ç–º–±–µ–¥–¥–∏–Ω–≥
    path_to_embedding = {}
    for emb, path in zip(embeddings, owners):
        path_to_embedding[path] = emb
    
    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –ø–∞—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
    cluster_ids = list(cluster_map.keys())
    merges_to_apply = []
    
    for i, cluster_id_i in enumerate(cluster_ids):
        paths_i = cluster_map[cluster_id_i]
        if len(paths_i) == 0:
            continue
            
        embeddings_i = [path_to_embedding[p] for p in paths_i if p in path_to_embedding]
        if not embeddings_i:
            continue
        centroid_i = np.mean(embeddings_i, axis=0)
        
        for j, cluster_id_j in enumerate(cluster_ids[i+1:], i+1):
            paths_j = cluster_map[cluster_id_j]
            if len(paths_j) == 0:
                continue
                
            embeddings_j = [path_to_embedding[p] for p in paths_j if p in path_to_embedding]
            if not embeddings_j:
                continue
            centroid_j = np.mean(embeddings_j, axis=0)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É —Ü–µ–Ω—Ç—Ä–æ–∏–¥–∞–º–∏
            dist = cosine_distances([centroid_i], [centroid_j])[0][0]
            
            # –°—É–ø–µ—Ä-–∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ - –æ–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –≤—Å–µ –ø–æ—Ö–æ–∂–∏–µ –ª–∏—Ü–∞
            if dist < SUPER_AGGRESSIVE_THRESHOLD:  # –û—á–µ–Ω—å –º—è–≥–∫–∏–π –ø–æ—Ä–æ–≥
                merges_to_apply.append((cluster_id_i, cluster_id_j))
                print(f"üî• –°—É–ø–µ—Ä-–∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ {cluster_id_i} –∏ {cluster_id_j} (—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {dist:.3f})")
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
    if merges_to_apply:
        print(f"üî• –ü—Ä–∏–º–µ–Ω—è–µ–º {len(merges_to_apply)} —Å—É–ø–µ—Ä-–∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–π...")
        final_cluster_map = cluster_map.copy()
        
        # –ü—Ä–æ—Å—Ç–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ - –æ–±—ä–µ–¥–∏–Ω—è–µ–º –ø–µ—Ä–≤—ã–π –∫–ª–∞—Å—Ç–µ—Ä —Å–æ –≤—Ç–æ—Ä—ã–º
        for cluster_a, cluster_b in merges_to_apply:
            if cluster_a in final_cluster_map and cluster_b in final_cluster_map:
                final_cluster_map[cluster_a].update(final_cluster_map[cluster_b])
                del final_cluster_map[cluster_b]
                print(f"üî• –°—É–ø–µ—Ä-–æ–±—ä–µ–¥–∏–Ω–∏–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä {cluster_b} —Å {cluster_a}")
        
        return final_cluster_map
    
    return cluster_map

def build_plan_live(
    input_dir: Path,
    det_size=(640, 640),
    min_score: float = 0.5,  # –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ –ª–∏—Ü
    min_cluster_size: int = 2,  # HDBSCAN —Ç—Ä–µ–±—É–µ—Ç –º–∏–Ω–∏–º—É–º 2 —ç–ª–µ–º–µ–Ω—Ç–∞
    min_samples: int = 1,       # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤
    providers: List[str] = ("CPUExecutionProvider",),
    progress_callback=None,
):
    input_dir = Path(input_dir)
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –∏—Å–∫–ª—é—á–∞—è —Ç–µ, —á—Ç–æ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –ø–∞–ø–∫–∞—Ö —Å –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–º–∏ –∏–º–µ–Ω–∞–º–∏
    excluded_names = ["–æ–±—â–∏–µ", "–æ–±—â–∞—è", "common", "shared", "–≤—Å–µ", "all", "mixed", "—Å–º–µ—à–∞–Ω–Ω—ã–µ"]
    all_images = [
        p for p in input_dir.rglob("*")
        if is_image(p)
        and not any(ex in str(p).lower() for ex in excluded_names)
    ]

    if progress_callback:
        progress_callback(f"üìÇ –°–∫–∞–Ω–∏—Ä—É–µ—Ç—Å—è: {input_dir}, –Ω–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(all_images)}", 1)

    app = FaceAnalysis(name="buffalo_l", providers=list(providers))
    ctx_id = -1 if "cpu" in str(providers).lower() else 0
    app.prepare(ctx_id=ctx_id, det_size=det_size)

    if progress_callback:
        progress_callback("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –Ω–∞—á–∏–Ω–∞–µ–º –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...", 10)

    embeddings = []
    owners = []
    img_face_count = {}
    unreadable = []
    no_faces = []

    total = len(all_images)
    processed_faces = 0
    
    for i, p in enumerate(all_images):
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        if progress_callback:
            percent = 10 + int((i + 1) / max(total, 1) * 70)  # 10-80% –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            progress_callback(f"üì∑ –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {percent}% ({i+1}/{total}) - {p.name}", percent)
        
        print(f"üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {i+1}/{total}: {p.name}")
        
        img = imread_safe(p)
        if img is None:
            print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {p.name}")
            unreadable.append(p)
            continue
            
        print(f"‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ, —Ä–∞–∑–º–µ—Ä: {img.shape}")
        faces = app.get(img)
        print(f"üîç –ù–∞–π–¥–µ–Ω–æ –ª–∏—Ü: {len(faces) if faces else 0}")
        
        if not faces:
            print(f"‚ö†Ô∏è –õ–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤: {p.name}")
            no_faces.append(p)
            continue

        count = 0
        for f in faces:
            if getattr(f, "det_score", 1.0) < min_score:
                continue
            emb = getattr(f, "normed_embedding", None)
            if emb is None:
                continue
            emb = emb.astype(np.float64)  # HDBSCAN expects double
            
            # –£–ª—É—á—à–µ–Ω–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            norm = np.linalg.norm(emb)
            if norm > 0:
                emb = emb / norm
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
                if np.any(np.isnan(emb)) or np.any(np.isinf(emb)):
                    continue
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç–º–±–µ–¥–¥–∏–Ω–≥ –Ω–µ —Å–ª–∏—à–∫–æ–º –±–ª–∏–∑–æ–∫ –∫ –Ω—É–ª—é
                if np.max(np.abs(emb)) < 1e-6:
                    continue
                    
            embeddings.append(emb)
            owners.append(p)
            count += 1
            processed_faces += 1

        if count > 0:
            img_face_count[p] = count
            print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –ª–∏—Ü –≤ {p.name}: {count}")

    print(f"üìä –ò—Ç–æ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(all_images)}")
    print(f"üìä –ù–∞–π–¥–µ–Ω–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {len(embeddings)}")
    print(f"üìä –ù–µ—á–∏—Ç–∞–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(unreadable)}")
    print(f"üìä –§–∞–π–ª–æ–≤ –±–µ–∑ –ª–∏—Ü: {len(no_faces)}")

    if not embeddings:
        if progress_callback:
            progress_callback("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ª–∏—Ü –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏", 100)
        print(f"‚ö†Ô∏è –ù–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {input_dir}")
        return {
            "clusters": {},
            "plan": [],
            "unreadable": [str(p) for p in unreadable],
            "no_faces": [str(p) for p in no_faces],
        }

    # –≠—Ç–∞–ø 2: –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    print(f"üîÑ –ù–∞—á–∏–Ω–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é {len(embeddings)} –ª–∏—Ü...")
    if progress_callback:
        progress_callback(f"üîÑ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è {len(embeddings)} –ª–∏—Ü...", 80)
    
    X = np.vstack(embeddings)
    print(f"üìê –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –¥–ª—è {X.shape[0]} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
    
    # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π
    if X.shape[0] > 50:
        print("‚ö†Ô∏è –ë–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º...")
        # –î–ª—è –±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥
        from sklearn.metrics.pairwise import cosine_similarity
        similarity_matrix = cosine_similarity(X)
        distance_matrix = 1 - similarity_matrix
    else:
        distance_matrix = cosine_distances(X)
    
    print(f"‚úÖ –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π —Å–æ–∑–¥–∞–Ω–∞: {distance_matrix.shape}")

    if progress_callback:
        progress_callback("üîÑ –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π...", 85)

    print("üîÑ –ó–∞–ø—É—Å–∫–∞–µ–º HDBSCAN...")
    # –ü—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–∞–π–º–∞—É—Ç —á–µ—Ä–µ–∑ signal, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
    try:
        import signal
        timeout_supported = hasattr(signal, 'SIGALRM') and hasattr(signal, 'alarm')
    except Exception:
        timeout_supported = False
    if timeout_supported:
        try:
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç
            def timeout_handler(signum, frame):
                raise TimeoutError("HDBSCAN timeout")
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(300)  # 5 –º–∏–Ω—É—Ç
            raw_labels = hdbscan.HDBSCAN(metric='precomputed', min_cluster_size=min_cluster_size, min_samples=min_samples).fit_predict(distance_matrix)
            signal.alarm(0)
            print(f"‚úÖ HDBSCAN —Å —Ç–∞–π–º–∞—É—Ç–æ–º –∑–∞–≤–µ—Ä—à–µ–Ω. –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –º–µ—Ç–∫–∏: {np.unique(raw_labels)}")
        except TimeoutError:
            print("‚ö†Ô∏è HDBSCAN timeout! –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—É—é –∞–≥–ª–æ–º–µ—Ä–∞—Ç–∏–≤–Ω—É—é –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é...")
            try:
                from sklearn.cluster import AgglomerativeClustering
                agg = AgglomerativeClustering(n_clusters=None, metric='precomputed', linkage='average', distance_threshold=0.35)
                raw_labels = agg.fit_predict(distance_matrix)
                print(f"‚úÖ AgglomerativeClustering –∑–∞–≤–µ—Ä—à–µ–Ω. –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –º–µ—Ç–∫–∏: {np.unique(raw_labels)}")
            except Exception as e2:
                print(f"‚ùå –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å: {e2}. –í—Å–µ –≤ –æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä.")
                raw_labels = np.zeros(len(embeddings), dtype=int)
    else:
        print("‚ÑπÔ∏è –¢–∞–π–º–∞—É—Ç HDBSCAN –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –¥–∞–Ω–Ω–æ–π –ø–ª–∞—Ç—Ñ–æ—Ä–º–µ, –∑–∞–ø—É—Å–∫–∞–µ–º –±–µ–∑ —Ç–∞–π–º–∞—É—Ç–∞...")
        try:
            raw_labels = hdbscan.HDBSCAN(metric='precomputed', min_cluster_size=min_cluster_size, min_samples=min_samples).fit_predict(distance_matrix)
            print(f"‚úÖ HDBSCAN –±–µ–∑ —Ç–∞–π–º–∞—É—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω. –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –º–µ—Ç–∫–∏: {np.unique(raw_labels)}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ HDBSCAN –±–µ–∑ —Ç–∞–π–º–∞—É—Ç–∞: {e}. –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—É—é –∞–≥–ª–æ–º–µ—Ä–∞—Ç–∏–≤–Ω—É—é –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é...")
            try:
                from sklearn.cluster import AgglomerativeClustering
                agg = AgglomerativeClustering(n_clusters=None, metric='precomputed', linkage='average', distance_threshold=0.35)
                raw_labels = agg.fit_predict(distance_matrix)
                print(f"‚úÖ AgglomerativeClustering –∑–∞–≤–µ—Ä—à–µ–Ω. –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –º–µ—Ç–∫–∏: {np.unique(raw_labels)}")
            except Exception as e2:
                print(f"‚ùå –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å: {e2}. –í—Å–µ –≤ –æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä.")
                raw_labels = np.zeros(len(embeddings), dtype=int)

    # Fallback: –µ—Å–ª–∏ HDBSCAN –ø–æ–º–µ—Ç–∏–ª –≤—Å–µ —Ç–æ—á–∫–∏ –∫–∞–∫ —à—É–º, –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã,
    # –∫–æ—Ç–æ—Ä—ã–µ –∑–∞—Ç–µ–º –±—É–¥—É—Ç —Å–ª–∏—Ç—ã –Ω–∞—à–∏–º–∏ —ç—Ç–∞–ø–∞–º–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
    if raw_labels.size > 0 and np.all(raw_labels == -1):
        if progress_callback:
            progress_callback("‚ö†Ô∏è –í—Å–µ —Ç–æ—á–∫–∏ –ø–æ–º–µ—á–µ–Ω—ã –∫–∞–∫ —à—É–º HDBSCAN. –í–∫–ª—é—á–∞–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—ã–π —Ä–µ–∂–∏–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏.", 82)
        raw_labels = np.arange(len(embeddings), dtype=int)

    cluster_map, cluster_by_img = merge_clusters_by_centroid(
        embeddings=embeddings,
        owners=owners,
        raw_labels=raw_labels,
        auto_threshold=True,
        margin=0.10,  # –ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
        min_threshold=0.18,  # –ë–æ–ª–µ–µ –º—è–≥–∫–∏–π –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
        max_threshold=0.45,  # –ë–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–π –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
        progress_callback=progress_callback
    )
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    cluster_map = post_process_clusters(
        cluster_map=cluster_map,
        embeddings=embeddings,
        owners=owners,
        progress_callback=progress_callback
    )
    
    # –§–∏–Ω–∞–ª—å–Ω–æ–µ —É–º–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –æ–¥–Ω–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞
    cluster_map = smart_final_merge(
        cluster_map=cluster_map,
        embeddings=embeddings,
        owners=owners,
        progress_callback=progress_callback
    )
    
    # –°—É–ø–µ—Ä-–∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∫–∞–∫ –ø–æ—Å–ª–µ–¥–Ω–∏–π —ç—Ç–∞–ø
    cluster_map = super_aggressive_merge(
        cluster_map=cluster_map,
        embeddings=embeddings,
        owners=owners,
        progress_callback=progress_callback
    )
    
    # –û–±–Ω–æ–≤–ª—è–µ–º cluster_by_img –ø–æ—Å–ª–µ –≤—Å–µ—Ö –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–π
    cluster_by_img = defaultdict(set)
    for cluster_id, paths in cluster_map.items():
        for path in paths:
            cluster_by_img[path].add(cluster_id)

    # –≠—Ç–∞–ø 3: –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–ª–∞–Ω–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    print(f"üîÑ –§–æ—Ä–º–∏—Ä—É–µ–º –ø–ª–∞–Ω —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è...")
    if progress_callback:
        progress_callback("üîÑ –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–ª–∞–Ω–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è...", 95)
    
    plan = []
    for path in all_images:
        clusters = cluster_by_img.get(path)
        if not clusters:
            continue
        plan.append({
            "path": str(path),
            "cluster": sorted(list(clusters)),
            "faces": img_face_count.get(path, 0)
        })
    
    print(f"üìã –ü–ª–∞–Ω —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è: {len(plan)} —Ñ–∞–π–ª–æ–≤")

    # –ï—Å–ª–∏ –ø–æ –∫–∞–∫–æ–π-—Ç–æ –ø—Ä–∏—á–∏–Ω–µ –ø–ª–∞–Ω –ø—É—Å—Ç, –Ω–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –±—ã–ª–∏ ‚Äî –ø–µ—Ä–µ–Ω–æ—Å–∏–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ª–∏—Ü–∞–º–∏ –≤ –æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä
    if not plan and embeddings:
        if progress_callback:
            progress_callback("‚ö†Ô∏è –ü–ª–∞–Ω –ø—É—Å—Ç. –ü–µ—Ä–µ–Ω–æ—Å–∏–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ª–∏—Ü–∞–º–∏ –≤ –æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä (—Ä–µ–∑–µ—Ä–≤–Ω—ã–π —Ä–µ–∂–∏–º)", 96)
        fallback_cluster_id = 0
        img_with_faces = [p for p, cnt in img_face_count.items() if cnt > 0]
        for p in img_with_faces:
            plan.append({
                "path": str(p),
                "cluster": [fallback_cluster_id],
                "faces": img_face_count.get(p, 0)
            })

    # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ
    if progress_callback:
        progress_callback(f"‚úÖ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –ù–∞–π–¥–µ–Ω–æ {len(cluster_map)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(plan)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", 100)

    print(f"‚úÖ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {input_dir} ‚Üí –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(cluster_map)}, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(plan)}")

    return {
        "clusters": {
            int(k): [str(p) for p in sorted(v, key=lambda x: str(x))]
            for k, v in cluster_map.items()
        },
        "plan": plan,
        "unreadable": [str(p) for p in unreadable],
        "no_faces": [str(p) for p in no_faces],
    }

def distribute_to_folders(plan: dict, base_dir: Path, cluster_start: int = 1, progress_callback=None) -> Tuple[int, int, int]:
    moved, copied = 0, 0
    moved_paths = set()

    used_clusters = sorted({c for item in plan.get("plan", []) for c in item["cluster"]})
    cluster_id_map = {old: cluster_start + idx for idx, old in enumerate(used_clusters)}

    plan_items = plan.get("plan", [])
    total_items = len(plan_items)
    
    if progress_callback:
        progress_callback(f"üîÑ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ {total_items} —Ñ–∞–π–ª–æ–≤ –ø–æ –ø–∞–ø–∫–∞–º...", 0)

    for i, item in enumerate(plan_items):
        if progress_callback:
            percent = int((i + 1) / max(total_items, 1) * 100)
            progress_callback(f"üìÅ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤: {percent}% ({i+1}/{total_items})", percent)
            
        src = Path(item["path"])
        clusters = [cluster_id_map[c] for c in item["cluster"]]
        if not src.exists():
            continue

        if len(clusters) == 1:
            cluster_id = clusters[0]
            dst = base_dir / f"{cluster_id}" / src.name
            dst.parent.mkdir(parents=True, exist_ok=True)
            try:
                shutil.move(str(src), str(dst))
                moved += 1
                moved_paths.add(src.parent)
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏—è {src} ‚Üí {dst}: {e}")
        else:
            for cluster_id in clusters:
                dst = base_dir / f"{cluster_id}" / src.name
                dst.parent.mkdir(parents=True, exist_ok=True)
                try:
                    shutil.copy2(str(src), str(dst))
                    copied += 1
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è {src} ‚Üí {dst}: {e}")
            try:
                src.unlink()  # —É–¥–∞–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª –ø–æ—Å–ª–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–∞–ø–æ–∫
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è {src}: {e}")

    # –û—á–∏—Å—Ç–∫–∞ –ø—É—Å—Ç—ã—Ö –ø–∞–ø–æ–∫
    if progress_callback:
        progress_callback("üßπ –û—á–∏—Å—Ç–∫–∞ –ø—É—Å—Ç—ã—Ö –ø–∞–ø–æ–∫...", 100)

    for p in sorted(moved_paths, key=lambda x: len(str(x)), reverse=True):
        try:
            if p.exists() and not any(p.iterdir()):
                p.rmdir()
        except Exception:
            pass

    print(f"üì¶ –ü–µ—Ä–µ–º–µ—â–µ–Ω–æ: {moved}, —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ: {copied}")
    return moved, copied, cluster_start + len(used_clusters)

def process_group_folder(group_dir: Path, progress_callback=None):
    cluster_counter = 1
    subfolders = [f for f in sorted(group_dir.iterdir()) if f.is_dir() and "–æ–±—â–∏–µ" not in f.name.lower()]
    total_subfolders = len(subfolders)

    for i, subfolder in enumerate(subfolders):
        # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ–¥–ø–∞–ø–æ–∫
        if progress_callback:
            percent = 10 + int((i + 1) / max(total_subfolders, 1) * 80)
            progress_callback(f"üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –ø–æ–¥–ø–∞–ø–∫–∞: {subfolder.name} ({i+1}/{total_subfolders})", percent)

        print(f"üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –ø–æ–¥–ø–∞–ø–∫–∞: {subfolder}")
        # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å –ø–µ—Ä–µ–¥–∞—á–µ–π –∫–æ–ª–ª–±—ç–∫–∞ –¥–ª—è –ª–æ–≥–æ–≤
        plan = build_plan_live(subfolder, progress_callback=progress_callback)
        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        clusters_count = len(plan.get('clusters', {}))
        items_count = len(plan.get('plan', []))
        print(f"üìä –ü–æ–¥–ø–∞–ø–∫–∞: {subfolder.name} ‚Üí –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {clusters_count}, —Ñ–∞–π–ª–æ–≤: {items_count}")
        if progress_callback:
            progress_callback(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç: –∫–ª–∞—Å—Ç–µ—Ä–æ–≤={clusters_count}, —Ñ–∞–π–ª–æ–≤={items_count}", percent=percent + 1)

        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ø–∞–ø–∫–∞–º —Å –ø–µ—Ä–µ–¥–∞—á–µ–π –∫–æ–ª–ª–±—ç–∫–∞
        moved, copied, cluster_counter = distribute_to_folders(
            plan,
            subfolder,
            cluster_start=cluster_counter,
            progress_callback=progress_callback
        )
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–Ω–æ—Å–∞
        print(f"üì¶ –ü–µ—Ä–µ–Ω–µ—Å–µ–Ω–æ: {moved}, —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ: {copied} –≤ –ø–æ–¥–ø–∞–ø–∫–µ {subfolder.name}")
        if progress_callback:
            progress_callback(f"üì¶ –ü–µ—Ä–µ–Ω–µ—Å–µ–Ω–æ={moved}, —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ={copied}", percent=90)



